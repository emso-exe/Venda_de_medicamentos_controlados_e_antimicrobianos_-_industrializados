{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Venda de Medicamentos Controlados e Antimicrobianos - Medicamentos Industrializados\n",
    "\n",
    "Projeto de análise de vendas de medicamentos controlados e antimicrobianos exclusivamente industrializados na Região Metropolitana da Baixada Santista (RMBS) composta por nove municípios no litoral do estado de São Paulo, através de dados extraídos do Sistema Nacional de Gerenciamento de Produtos Controlados (SNGPC) e disponibilizados no [portal de dados abertos](https://dados.gov.br/dados/conjuntos-dados/venda-de-medicamentos-controlados-e-antimicrobianos---medicamentos-industrializados) da Agência Nacional de Vigilância Sanitária (Anvisa). Através da análise das informações de vendas, dados geográricos, perfil de pacientes e características de medicamentos, o objetivo é extrair insights de negócios, sugerir soluções aos problemas identificados e apresentar propostas de aprimoramento.\n",
    "\n",
    "## Demanda do negócio\n",
    "\n",
    "- Compreender as tendências, padrões e características das vendas farmacêuticas.\n",
    "- Construção de perfis de pacientes com base nos mendicamentos receitados.\n",
    "- Compreender a demanda de medicamentos com base no perfil dos pacientes, por tempo e por município.\n",
    "- Obter insights, identificar oportunidades e propor soluções a problemas.\n",
    "- Elaboração de painel de informações de medicamentos baseado em filtros.\n",
    "\n",
    "## Compreensão dos dados\n",
    "\n",
    "Os dados que serão utilizados na análise compreendem o período de uma ano, outubro de 2020 até setembro de 2021, que integram um conjunto de doze arquivos em formato \"CSV\". Os dados foram extraídos do Sistema Nacional de Gerenciamento de Produtos Controlados (SNGPC), provenientes apenas de farmácias e drogarias privadas que periodicamente devem enviar os dados a respeito de todas as vendas realizadas de medicamentos sujeitos à escrituração no SNGPC. Os dados foram disponibilizados no [portal de dados abertos](https://dados.gov.br/dados/conjuntos-dados/venda-de-medicamentos-controlados-e-antimicrobianos---medicamentos-industrializados) da Agência Nacional de Vigilância Sanitária (Anvisa).\n",
    "\n",
    "\n",
    "## Tópico da análise\n",
    "\n",
    "- Construir uma ABT (analytical base table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, desc, col\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação e iniciação de uma sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://rootx:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark - ABT de Vendas Farmaceuticas</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1fe95edc690>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appName = 'PySpark - ABT de Vendas Farmaceuticas'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .config('spark.driver.memory', '8g') \\\n",
    "    .config('spark.driver.cores', '2') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do dataset a partir da leitura dos arquivos *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ANO_VENDA: integer (nullable = true)\n",
      " |-- MES_VENDA: integer (nullable = true)\n",
      " |-- UF_VENDA: string (nullable = true)\n",
      " |-- MUNICIPIO_VENDA: string (nullable = true)\n",
      " |-- PRINCIPIO_ATIVO: string (nullable = true)\n",
      " |-- DESCRICAO_APRESENTACAO: string (nullable = true)\n",
      " |-- QTD_VENDIDA: integer (nullable = true)\n",
      " |-- UNIDADE_MEDIDA: string (nullable = true)\n",
      " |-- CONSELHO_PRESCRITOR: string (nullable = true)\n",
      " |-- UF_CONSELHO_PRESCRITOR: string (nullable = true)\n",
      " |-- TIPO_RECEITUARIO: decimal(1,0) (nullable = true)\n",
      " |-- CID10: string (nullable = true)\n",
      " |-- SEXO: integer (nullable = true)\n",
      " |-- IDADE: double (nullable = true)\n",
      " |-- UNIDADE_IDADE: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lista de caminhos para os arquivos CSV\n",
    "caminhos_csv = glob.glob('dados/*.csv')\n",
    "\n",
    "# DataFrame vazio\n",
    "df = None\n",
    "\n",
    "# Loop para ler e unir os arquivos CSV\n",
    "for caminho_csv in caminhos_csv:\n",
    "    df_temp = spark.read.csv(caminho_csv, sep=';', header=True, encoding='latin1', inferSchema=True)\n",
    "    \n",
    "    if df is None:\n",
    "        df = df_temp\n",
    "    else:\n",
    "        df = df.union(df_temp)\n",
    "\n",
    "# Exibir o esquema do DataFrame combinado\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------------+------------------------+---------------------------------------------------------+-----------+--------------+-------------------+----------------------+----------------+-----+----+-----+-------------+\n",
      "|ANO_VENDA|MES_VENDA|UF_VENDA|MUNICIPIO_VENDA|PRINCIPIO_ATIVO         |DESCRICAO_APRESENTACAO                                   |QTD_VENDIDA|UNIDADE_MEDIDA|CONSELHO_PRESCRITOR|UF_CONSELHO_PRESCRITOR|TIPO_RECEITUARIO|CID10|SEXO|IDADE|UNIDADE_IDADE|\n",
      "+---------+---------+--------+---------------+------------------------+---------------------------------------------------------+-----------+--------------+-------------------+----------------------+----------------+-----+----+-----+-------------+\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM              |6 MG CAP GEL C/ MCGRAN AP CT BL AL PVC ACLAR PL X 30     |3          |FRASCO        |CRM                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM              |6 MG COM CT BL AL PLAS INC X 30                          |22         |CAIXA         |CRM                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM              |6 MG COM CT BL AL PLAS INC X 30                          |3          |CAIXA         |RMS                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM              |6 MG COM CT BL AL PLAS INC X 30                          |1          |FRASCO        |CRM                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM              |6 MG COM CT BL AL PLAS TRANS X 30                        |24         |CAIXA         |CRM                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM              |6 MG COM CT BL AL PLAS TRANS X 30                        |228        |FRASCO        |CRM                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM              |6 MG COM CT BL AL PLAS TRANS X 30                        |2          |FRASCO        |RMS                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM              |6,0 MG COM CT BL AL PLAS INC X 30                        |2          |FRASCO        |CRM                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM              |6,0 MG COM CT BL AL PLAS TRANS X 30                      |9          |CAIXA         |CRM                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM              |6,0 MG COM CT BL AL PLAS TRANS X 30                      |10         |FRASCO        |CRM                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM + SULPIRIDA  |(1,00+25,00) MG CAP GEL DURA CT BL AL PLAS PVC TRANS X 20|2          |FRASCO        |CRM                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM + SULPIRIDA  |1 MG + 25 MG CAP GEL DURA CT BL AL PLAS INC X 20         |3          |FRASCO        |CRM                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMAZEPAM + SULPIRIDA  |1 MG + 25 MG CAP GEL DURA CT BL AL PLAS TRANS X 20       |5          |CAIXA         |CRM                |BA                    |2               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMIDRATO DE CITALOPRAM|20 MG COM REV CT BL AL PLAS INC X 28 CALEND              |3          |CAIXA         |CRM                |BA                    |1               |NULL |NULL|NULL |NULL         |\n",
      "|2020     |10       |BA      |JEQUIÉ         |BROMIDRATO DE CITALOPRAM|20 MG COM REV CT BL AL PLAS PVDC OPC X 30                |3          |CAIXA         |CRM                |BA                    |3               |NULL |NULL|NULL |NULL         |\n",
      "+---------+---------+--------+---------------+------------------------+---------------------------------------------------------+-----------+--------------+-------------------+----------------------+----------------+-----+----+-----+-------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exibir as 5 primeiras linhas do DataFrame combinado\n",
    "df.show(15, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O DataFrame tem 73950048 linhas.\n",
      "\n",
      "O DataFrame tem 15 colunas.\n"
     ]
    }
   ],
   "source": [
    "# Contar o número de linhas no DataFrame\n",
    "qtde_linhas = df.count()\n",
    "print(f'\\nO DataFrame tem {qtde_linhas} linhas.')\n",
    "\n",
    "# Obter o número de colunas no DataFrame\n",
    "qtde_colunas = len(df.columns)\n",
    "print(f'\\nO DataFrame tem {qtde_colunas} colunas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos dados para construção da ABT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma View temporária para uso do SparkSQL\n",
    "df.createOrReplaceTempView('tb_medicamentos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|QTDE_TOTAL|\n",
      "+----------+\n",
      "|    748950|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificando a volume total de registros dos 9 municípios da RMBS\n",
    "qtde_linhas_rmbs = spark.sql('''\n",
    "    SELECT\n",
    "        COUNT(*) AS QTDE_TOTAL\n",
    "    FROM \n",
    "        tb_medicamentos\n",
    "    WHERE\n",
    "        MUNICIPIO_VENDA IN ('BERTIOGA', 'CUBATÃO', 'GUARUJÁ', 'ITANHAÉM', 'MONGAGUÁ', 'PERUÍBE', 'PRAIA GRANDE', 'SANTOS', 'SÃO VICENTE')\n",
    "''')\n",
    "\n",
    "qtde_linhas_rmbs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+------------------+\n",
      "|MUNICIPIO_VENDA|QTDE_TOTAL|QTDE_TOTAL_PERCENT|\n",
      "+---------------+----------+------------------+\n",
      "|         SANTOS|    214327|            28.617|\n",
      "|   PRAIA GRANDE|    135360|            18.073|\n",
      "|    SÃO VICENTE|    108158|            14.441|\n",
      "|        GUARUJÁ|     94778|            12.655|\n",
      "|        PERUÍBE|     44902|             5.995|\n",
      "|       ITANHAÉM|     42737|             5.706|\n",
      "|        CUBATÃO|     41304|             5.515|\n",
      "|       BERTIOGA|     34645|             4.626|\n",
      "|       MONGAGUÁ|     32739|             4.371|\n",
      "+---------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Coletar o resultado da primeira consulta\n",
    "qtde_rmbs = qtde_linhas_rmbs.first()['QTDE_TOTAL']\n",
    "\n",
    "# Verificando a volume total de registros de cada 1 dos 9 municípios da RMBS\n",
    "spark.sql('''\n",
    "    SELECT\n",
    "        MUNICIPIO_VENDA,\n",
    "        COUNT(*) AS QTDE_TOTAL,\n",
    "        ROUND(100*(COUNT(*)/{}), 3) AS QTDE_TOTAL_PERCENT\n",
    "    FROM \n",
    "        tb_medicamentos\n",
    "    WHERE\n",
    "        MUNICIPIO_VENDA IN ('BERTIOGA', 'CUBATÃO', 'GUARUJÁ', 'ITANHAÉM', 'MONGAGUÁ', 'PERUÍBE', 'PRAIA GRANDE', 'SANTOS', 'SÃO VICENTE')\n",
    "    GROUP BY\n",
    "        MUNICIPIO_VENDA\n",
    "    ORDER BY \n",
    "        QTDE_TOTAL DESC\n",
    "'''.format(qtde_rmbs)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------------+--------------------------------------------------------+-----------------------------------------------------+-----------+--------------+-------------------+----------------------+----------------+-----+----+-----+-------------+----------+----------+\n",
      "|ANO_VENDA|MES_VENDA|UF_VENDA|MUNICIPIO_VENDA|PRINCIPIO_ATIVO                                         |DESCRICAO_APRESENTACAO                               |QTD_VENDIDA|UNIDADE_MEDIDA|CONSELHO_PRESCRITOR|UF_CONSELHO_PRESCRITOR|TIPO_RECEITUARIO|CID10|SEXO|IDADE|UNIDADE_IDADE|DATA_REF  |DATA_PROC |\n",
      "+---------+---------+--------+---------------+--------------------------------------------------------+-----------------------------------------------------+-----------+--------------+-------------------+----------------------+----------------+-----+----+-----+-------------+----------+----------+\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |1   |69.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |1   |73.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |1   |78.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |2   |18.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |2   |38.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |2   |66.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |2   |74.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |2          |CAIXA         |CRMV               |SP                    |5               |NULL |NULL|NULL |NULL         |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |1          |FRASCO        |CRM                |SP                    |1               |NULL |1   |75.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |1          |FRASCO        |CRM                |SP                    |5               |NULL |2   |22.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |1          |FRASCO        |CRM                |SP                    |5               |NULL |2   |60.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 3 ML |1          |FRASCO        |CRMV               |SP                    |1               |NULL |NULL|NULL |NULL         |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 6 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |1   |48.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 6 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |1   |63.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 6 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |1   |68.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 6 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |1   |69.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 6 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |1   |72.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 6 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |1   |75.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 6 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |1   |77.0 |1            |2020-10-01|2023-10-07|\n",
      "|2020     |10       |SP      |MONGAGUÁ       |ACETATO DE PREDNISOLONA + GATIFLOXACINO SESQUI-HIDRATADO|3 MG/ML + 10 MG/ML SUS OFT CT FR GOT PLAS OPC X 6 ML |1          |CAIXA         |CRM                |SP                    |5               |NULL |1   |79.0 |1            |2020-10-01|2023-10-07|\n",
      "+---------+---------+--------+---------------+--------------------------------------------------------+-----------------------------------------------------+-----------+--------------+-------------------+----------------------+----------------+-----+----+-----+-------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionando os dados que irão compor a ABT\n",
    "abt_rmbs = spark.sql('''\n",
    "    SELECT\n",
    "        ANO_VENDA,\n",
    "        MES_VENDA,\n",
    "        UF_VENDA,\n",
    "        MUNICIPIO_VENDA,\n",
    "        PRINCIPIO_ATIVO,\n",
    "        DESCRICAO_APRESENTACAO,\n",
    "        QTD_VENDIDA,\n",
    "        UNIDADE_MEDIDA,\n",
    "        CONSELHO_PRESCRITOR,\n",
    "        UF_CONSELHO_PRESCRITOR,\n",
    "        TIPO_RECEITUARIO,\n",
    "        CID10,\n",
    "        SEXO,\n",
    "        IDADE,\n",
    "        UNIDADE_IDADE,\n",
    "        TO_DATE(CONCAT(ANO_VENDA, MES_VENDA),'yyyyMM') AS DATA_REF,\n",
    "        CURRENT_DATE AS DATA_PROC \n",
    "    FROM\n",
    "        tb_medicamentos\n",
    "    WHERE\n",
    "        MUNICIPIO_VENDA IN ('BERTIOGA', 'CUBATÃO', 'GUARUJÁ', 'ITANHAÉM', 'MONGAGUÁ', 'PERUÍBE', 'PRAIA GRANDE', 'SANTOS', 'SÃO VICENTE')    \n",
    "''')\n",
    "\n",
    "abt_rmbs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_ROUTINE] Cannot resolve function `replicate` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`].; line 4 pos 14",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\PROJETOS\\analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados\\000_ABT.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m teste \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49msql(\u001b[39m'''\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m    SELECT\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m        ANO_VENDA,\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m        right(replicate(\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,2) + convert(VARCHAR,MES_VENDA),2),\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m        TO_DATE(CONCAT(ANO_VENDA, MES_VENDA),\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39myyyyMM\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m) AS DATA_REF,\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m        CURRENT_DATE AS DATA_PROC \u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m    FROM\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m        tb_medicamentos\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    WHERE\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m        MUNICIPIO_VENDA IN (\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mBERTIOGA\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mCUBATÃO\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mGUARUJÁ\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mITANHAÉM\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMONGAGUÁ\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPERUÍBE\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPRAIA GRANDE\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSANTOS\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSÃO VICENTE\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m) and\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m        ANO_VENDA = \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m2021\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m   \u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m'''\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m teste\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m (args \u001b[39mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery, litArgs), \u001b[39mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_ROUTINE] Cannot resolve function `replicate` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`].; line 4 pos 14"
     ]
    }
   ],
   "source": [
    "teste = spark.sql('''\n",
    "    SELECT\n",
    "        ANO_VENDA,\n",
    "        right(replicate('0',2) + convert(VARCHAR,MES_VENDA),2),\n",
    "        TO_DATE(CONCAT(ANO_VENDA, MES_VENDA),'yyyyMM') AS DATA_REF,\n",
    "        CURRENT_DATE AS DATA_PROC \n",
    "    FROM\n",
    "        tb_medicamentos\n",
    "    WHERE\n",
    "        MUNICIPIO_VENDA IN ('BERTIOGA', 'CUBATÃO', 'GUARUJÁ', 'ITANHAÉM', 'MONGAGUÁ', 'PERUÍBE', 'PRAIA GRANDE', 'SANTOS', 'SÃO VICENTE') and\n",
    "        ANO_VENDA = '2021'   \n",
    "''')\n",
    "\n",
    "teste.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ANO_VENDA: integer (nullable = true)\n",
      " |-- MES_VENDA: integer (nullable = true)\n",
      " |-- UF_VENDA: string (nullable = true)\n",
      " |-- MUNICIPIO_VENDA: string (nullable = true)\n",
      " |-- PRINCIPIO_ATIVO: string (nullable = true)\n",
      " |-- DESCRICAO_APRESENTACAO: string (nullable = true)\n",
      " |-- QTD_VENDIDA: integer (nullable = true)\n",
      " |-- UNIDADE_MEDIDA: string (nullable = true)\n",
      " |-- CONSELHO_PRESCRITOR: string (nullable = true)\n",
      " |-- UF_CONSELHO_PRESCRITOR: string (nullable = true)\n",
      " |-- TIPO_RECEITUARIO: decimal(1,0) (nullable = true)\n",
      " |-- CID10: string (nullable = true)\n",
      " |-- SEXO: integer (nullable = true)\n",
      " |-- IDADE: double (nullable = true)\n",
      " |-- UNIDADE_IDADE: integer (nullable = true)\n",
      " |-- DATA_REF: date (nullable = true)\n",
      " |-- DATA_PROC: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exibir o esquema da ABT\n",
    "abt_rmbs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando a ABT em formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o469.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 21 in stage 119.0 failed 1 times, most recent failure: Lost task 21.0 in stage 119.0 (TID 1479) (rootx executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '20211' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.sort_addToSorter_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.time.format.DateTimeParseException: Text '20211' could not be parsed at index 4\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2108)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1936)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '20211' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.sort_addToSorter_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.time.format.DateTimeParseException: Text '20211' could not be parsed at index 4\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2108)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1936)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32me:\\PROJETOS\\analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados\\000_ABT.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m caminho \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdados/ABT/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#abt_rmbs.write.parquet(caminho, mode='overwrite')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#abt.write.partitionBy(\"PK_DATREF\").parquet(\"/content/drive/Shareddrives/PoD Academy/Cursos/Formação em Ciência de Dados/dados/sinteticos/ABT\")\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/PROJETOS/analise_de_venda_de_medicamentos_controlados_e_antimicrobianos_-_industrializados/000_ABT.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m abt_rmbs\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mpartitionBy(\u001b[39m'\u001b[39;49m\u001b[39mDATA_REF\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mparquet(caminho, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1719\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1720\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[1;32m-> 1721\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o469.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 21 in stage 119.0 failed 1 times, most recent failure: Lost task 21.0 in stage 119.0 (TID 1479) (rootx executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '20211' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.sort_addToSorter_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.time.format.DateTimeParseException: Text '20211' could not be parsed at index 4\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2108)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1936)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '20211' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.sort_addToSorter_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.time.format.DateTimeParseException: Text '20211' could not be parsed at index 4\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2108)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1936)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "# Convertendo a ABT (sql) para o formato 'parquet'\n",
    "caminho = 'dados/ABT/'\n",
    "#abt_rmbs.write.parquet(caminho, mode='overwrite')\n",
    "#abt.write.partitionBy(\"PK_DATREF\").parquet(\"/content/drive/Shareddrives/PoD Academy/Cursos/Formação em Ciência de Dados/dados/sinteticos/ABT\")\n",
    "abt_rmbs.write.partitionBy('DATA_REF').parquet(caminho, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validando a quantidade de linhas\n",
    "read_abt_rmbs = spark.read.format('parquet').load(caminho)\n",
    "\n",
    "print(f'\\nA ABT \\'parquet\\' tem {read_abt_rmbs.count()} linhas.')\n",
    "print(f'\\nA ABT \\'sql\\' tem {qtde_rmbs} linhas.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
